# app.py
import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import matplotlib.pyplot as plt
import pandas as pd
import random

# -----------------------------
# åˆå§‹åŒ–é¡µé¢è®¾ç½®
# -----------------------------
st.set_page_config(page_title="NLP æ¨¡å‹å¯è§†åŒ–ç³»ç»Ÿ", layout="wide")
st.title("ğŸ” åŸºäº DistilBERT çš„ NLP æ¨¡å‹å¯è§†åŒ– Demo")

# -----------------------------
# ç¤ºä¾‹æ•°æ®é›†
# -----------------------------
def load_sample_data():
    return pd.DataFrame({
        "text": [
            "I really loved the movie!",
            "The product was terrible and disappointing.",
            "It was okay, not great.",
            "Fantastic experience, would recommend!",
            "Worst decision ever.",
            "Nothing special, average performance.",
            "Highly enjoyable, brilliant acting.",
            "Awful, just awful.",
            "A masterpiece of cinema!",
            "Wouldnâ€™t watch it again."
        ]
    })

# -----------------------------
# æ¨¡å‹åŠ è½½
# -----------------------------
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
    model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
    return tokenizer, model

tokenizer, model = load_model()

# -----------------------------
# æ–‡æœ¬è¾“å…¥æˆ–æ ·æœ¬é€‰æ‹©
# -----------------------------
st.sidebar.header("ğŸ“Œ æ•°æ®è¾“å…¥")
mode = st.sidebar.radio("é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š", ["è‡ªç”±è¾“å…¥", "ç¤ºä¾‹æ ·æœ¬"])

if mode == "è‡ªç”±è¾“å…¥":
    text = st.text_area("è¯·è¾“å…¥è‹±æ–‡æ–‡æœ¬ï¼š", "I really enjoyed the film!")
else:
    df = load_sample_data()
    idx = st.sidebar.number_input("é€‰æ‹©æ ·æœ¬ç¼–å·", min_value=0, max_value=len(df)-1, step=1)
    text = df.iloc[idx]["text"]
    st.info(f"é€‰ä¸­æ–‡æœ¬ï¼š{text}")

# -----------------------------
# è¿è¡Œæ¨¡å‹å¹¶æ˜¾ç¤ºç»“æœ
# -----------------------------
if st.button("å¼€å§‹åˆ†æ"):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze()

    labels = ["Negative", "Positive"]
    pred_label = labels[probs.argmax().item()]

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ğŸ§¾ æ¨¡å‹é¢„æµ‹ç»“æœ")
        st.write(f"**é¢„æµ‹æ ‡ç­¾ï¼š** {pred_label}")
        st.write(f"**åˆ†ç±»æ¦‚ç‡ï¼š** {dict(zip(labels, [round(float(p), 3) for p in probs]))}")

        fig, ax = plt.subplots()
        ax.bar(labels, probs.tolist(), color=["red", "green"])
        ax.set_ylim([0, 1])
        st.pyplot(fig)

    with col2:
        st.subheader("ğŸ§  æ¨¡æ‹Ÿ Attention é«˜äº®")
        tokens = tokenizer.tokenize(text)
        importance = torch.rand(len(tokens))  # éšæœºæ¨¡æ‹Ÿæ³¨æ„åŠ›å€¼
        highlighted = ""
        for token, score in zip(tokens, importance):
            token_clean = token.replace("##", "")
            opacity = round(float(score), 2)
            highlighted += f"<span style='background-color: rgba(255,255,0,{opacity}); padding:2px'>{token_clean}</span> "
        st.markdown(highlighted, unsafe_allow_html=True)

# -----------------------------
# åº•éƒ¨ä¿¡æ¯
# -----------------------------
st.markdown("---")
st.caption("ğŸ¯ æœ¬å¹³å°ç”± Streamlit + HuggingFace Transformers æ„å»ºï¼Œå½“å‰ä¸º Demo ç‰ˆæœ¬ï¼Œä»…ç”¨äºæ•™å­¦ç”¨é€”ã€‚")

